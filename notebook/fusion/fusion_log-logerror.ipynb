{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Fusion des fichiers logs et logs error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'objectif est d'observer le nombre d'opérations réussies et d'erreur par heure, sur la période de temps disponible (aout-novembre 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et préparation du fichier log error"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ouverture du fichier\n",
    "df_error = pd.read_csv('../Data/logEtl/241016_LogETLError.csv', sep=';', dtype={'Program_Id': str, 'Schedules_Id': str, 'Schedules_Name': str})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_error.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Conversion de la colonne ETL_StartDateTime en Datetime pandas\n",
    "df_error['ETL_StartDateTime']=pd.to_datetime(df_error['ETL_StartDateTime'], format=\"%Y-%m-%d %H:%M:%S\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Récupération de la date de début et de fin du fichier\n",
    "\n",
    "start_date = df_error['ETL_StartDateTime'].min()\n",
    "end_date = df_error['ETL_StartDateTime'].max()\n",
    "\n",
    "print(\"Date la plus ancienne :\", start_date)\n",
    "print(\"Date la plus récente :\", end_date)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ajout d'une colonne Date et heure ne tenant pas compte des minutes\n",
    "df_error[\"Date et heure\"] = df_error[\"ETL_StartDateTime\"].dt.floor(\"h\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_error.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Catégorisation des messages d'erreur"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lister toutes les valeurs distinctes dans la colonne 'Message'\n",
    "distinct_types = df_error['Message'].unique()\n",
    "print(distinct_types)\n",
    "print(len(distinct_types))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def categorize_message(message):\n",
    "    message = message.lower()\n",
    "    if \"Guidez Atelier\".lower() in message or \"Guidez Social\".lower()  in message:\n",
    "        return \"Guidez Atelier Error\"\n",
    "    elif \"La taille du champ\".lower()  in message:\n",
    "        return \"Field Size Error\" \n",
    "    elif \"Un incident est survenu Object reference not set to an instance of an object.\".lower()  in message:\n",
    "        return \"Object Reference Error\"\n",
    "    elif \"La valeur #REF!\".lower()  in message:\n",
    "        return \"Value #REF! Error\"\n",
    "    elif \"La valeur #VALUE!\".lower()  in message:\n",
    "        return \"Value #VALUE! Error\"\n",
    "    elif \"La valeur #N/A!\".lower()  in message:\n",
    "        return \"Value #N/A! Error\"\n",
    "    elif \"La valeur #DIV/0!\".lower()  in message:\n",
    "        return \"Value #DIV/0! Error\"\n",
    "    elif \"La valeur #NAME?\".lower()  in message:\n",
    "        return \"Value #NAME? Error\"\n",
    "    elif \"Connection Timeout Expired.  The timeout period elapsed during the post-login phase.\".lower()  in message:\n",
    "        return \"Connection Timeout Error\"\n",
    "    elif \"an error occurred during the pre-login handshake.\".lower()  in message:\n",
    "        return \"Pre-Login Error\"\n",
    "    elif \"Un incident est survenu L'instruction CREATE UNIQUE INDEX a été interrompue, car une clé dupliquée a été trouvée pour l'objet\".lower()  in message:\n",
    "        return \"Unique Index Error\"\n",
    "    elif \"Impossible de créer la table\".lower()  in message:\n",
    "        return \"Create TABLE Error\"\n",
    "    elif \"Un incident est survenu Échec de l'opération car un index ou des statistiques portant le nom\".lower()  in message:\n",
    "        return \"Index Error (similar to unique index error)\"\n",
    "    elif \"utilisé dans la clé primaire, sa valeur ne doit pas être vide\".lower()  in message:\n",
    "        return \"Primary Key Error\"\n",
    "    elif \"pas convertible en Heure\".lower()  in message:\n",
    "        return \"Time Format Error\"\n",
    "    elif \"Not a legal OleAut date.\".lower()  in message or (\"La date\".lower()  in message and \"est pas valide\".lower()  in message):\n",
    "        return \"Date Format Error\"\n",
    "    elif \"connexion au web service impossible\".lower()  in message:\n",
    "        return \"Web Service Error\"\n",
    "    elif \"impossible d'ouvrir la requête\".lower()  in message and \"Un incident est survenu\".lower()  in message and \"non valide\".lower()  in message:\n",
    "        return \"SQL Invalid Query Error\"\n",
    "    elif \"Impossible d\\'ouvrir la requête SQL sur la connexion\".lower()  in message and \"Le délai d\\'attente a été dépassé\".lower()  in message:\n",
    "        return \"SQL Query Timeout Error\"\n",
    "    #elif \"Le filtre de suppression n'est pas compatible SQL\".lower()  in message:\n",
    "    #    return \"SQL Delete Filter Error\"\n",
    "    #elif \"impossible d'ouvrir la requête sql\".lower()  in message:\n",
    "     #   return \"SQL Other Error\"\n",
    "    elif \"Le nom de colonne\".lower()  in message and \"n'existe pas dans la table ou la vue cible\".lower()  in message:\n",
    "        return \"Column Name Error\"\n",
    "    else:\n",
    "        return \"Other Error\"\n",
    "\n",
    "df_error = df_error.assign(\n",
    "    Message_Category=df_error[\"Message\"].apply(categorize_message)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_error.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "message_count = df_error['Message_Category'].value_counts()\n",
    "print(message_count)\n",
    "print(len(message_count))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "message_count = df_error['Message_Category'].value_counts()\n",
    "plt.figure(figsize=(12, 8))\n",
    "message_count.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Message Category Counts\", fontsize=16)\n",
    "plt.xlabel(\"Message Category\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Détail sur les messages catégorisés dans \"Other Error\""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "unique_other_errors = df_error[df_error['Message_Category'] == \"Other Error\"]['Message'].unique()\n",
    "print(unique_other_errors)\n",
    "print(len(unique_other_errors))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A retenir : dans \"other error\" il y a 4 messages d'erreurs différents.On les néglige pour l'instant"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Opération de Groupby "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_error.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# choix des colonnes en vérifiant qu'il n'y ait pas de valeurs NaN (trop de problème dans la jointure après)\n",
    "nan_count = df_error.isna().sum()\n",
    "print(nan_count)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Je voulais utiliser Schedules_name, mais trop de valeurs NaN, donc je me rabats sur Program_Name. On va aussi aggréger sur Message_category évidement"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# liste des Message_Category\n",
    "print(df_error[\"Message_Category\"].unique())\n",
    "print(df_error[\"Message_Category\"].unique().shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# liste des Program_Name\n",
    "print(df_error[\"Program_Name\"].unique())\n",
    "print(df_error[\"Program_Name\"].unique().shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Utiliser des crosstab (ou pivot) et joindre les DataFrames\n",
    "\n",
    "# Comptage des catégories par Date et heure\n",
    "df_cat = pd.crosstab(df_error[\"Date et heure\"], df_error[\"Message_Category\"])\n",
    "\n",
    "# Comptage des programmes par Date et heure\n",
    "df_prog = pd.crosstab(df_error[\"Date et heure\"], df_error[\"Program_Name\"])\n",
    "\n",
    "# Joindre les deux sur l'index (qui est \"Date et heure\" dans les 2 crosstabs)\n",
    "df_error_grouped2 = df_cat.join(df_prog, how=\"outer\")\n",
    "\n",
    "# Remettre \"Date et heure\" en colonne si besoin\n",
    "df_error_grouped2 = df_error_grouped2.reset_index()\n",
    "\n",
    "df_error_grouped2.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Vérifier les colonnes Error\n",
    "error_columns = [col for col in df_error_grouped2.columns if \"Error\" in col]\n",
    "print(error_columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sommer le nb d'erreurs dans les colonnes erreur pour la vérification\n",
    "df_error_grouped2[\"Total_Errors\"] = df_error_grouped2[error_columns].sum(axis=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df_error_grouped2.shape)\n",
    "df_error_grouped2.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ancienne opération Groupby de Guillaume"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Group by la colonne Date et heure en faisant le nombre d'opérations\n",
    "df_error_grouped = df_error.groupby(\"Date et heure\").agg(\n",
    "    nb_operations_error=(\"Message_Category\", \"count\")\n",
    ").reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pourquoi????? \n",
    "#df_error_grouped = df_error_grouped.iloc[1:] "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_error_grouped.shape)\n",
    "df_error_grouped.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## vérification"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = df_error_grouped2.merge(df_error_grouped[[\"Date et heure\", \"nb_operations_error\"]], on=\"Date et heure\", how=\"inner\")\n",
    "result[\"Check\"] = result[\"Total_Errors\"] == result[\"nb_operations_error\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(result.shape)\n",
    "result.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result_false_check = result[result[\"Check\"] == False]\n",
    "print(result_false_check[\"Date et heure\"].unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Taille de resulat=false est de 0, c'est good"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Import et préparation du fichier log"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ouverture du fichier\n",
    "#df = pd.read_parquet(\"/Users/guillaumeramirez/OneDrive - CentraleSupelec/Smart ETL - DOR/data/parquet/LogETL_20250130.csv_sub_2501301744.parquet\")\n",
    "#df = pd.read_parquet('/Users/Antoine/Library/CloudStorage/OneDrive-CentraleSupelec/Smart ETL - DOR/data/parquet/LogETL_20250130.csv_sub_2501301744.parquet')\n",
    "df = pd.read_parquet(\"../../data/logParquet/LogETL_20250130.csv_sub_2501301744.parquet\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Conversion de la colonne ETL_StartDateTime en Datetime pandas\n",
    "df['ETL_StartDateTime']=pd.to_datetime(df['ETL_StartDateTime'], format=\"%d/%m/%Y %H:%M\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Réduction du df pour qu'il soit dans la bonne période\n",
    "df_reduced = df[(df['ETL_StartDateTime'] >= start_date) & (df['ETL_StartDateTime'] <= end_date)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_reduced.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ajout d'une colonne Date et heure ne tenant pas compte des minutes\n",
    "#df_reduced[\"Date et heure\"] = df_reduced[\"ETL_StartDateTime\"].dt.floor(\"h\")\n",
    "df_reduced.loc[:, \"Date et heure\"] = df_reduced[\"ETL_StartDateTime\"].dt.floor(\"h\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_reduced.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_reduced.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grouped = df_reduced.groupby(\"Date et heure\").agg(\n",
    "    nb_operations=(\"Insert mode\", \"count\"),  # Nombre total de lignes dans l'heure\n",
    "    rows_added=(\"Rows added\", \"sum\"),      # Somme des lignes ajoutées\n",
    "    rows_updated=(\"Rows updated\", \"sum\"),  # Somme des mises à jour\n",
    "    rows_deleted=(\"Rows deleted\", \"sum\")   # Somme des suppressions\n",
    ").reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grouped.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grouped.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion des deux fichiers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fusion des deux fichiers\n",
    "df_final = df_error_grouped2.merge(\n",
    "    df_grouped,\n",
    "    on=\"Date et heure\",\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "# Changement des valeurs NaN en 0\n",
    "df_final = df_final.fillna(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_final.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_final.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionel pour visualisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_filtered = df_final[df_final[\"Date et heure\"].dt.date == pd.to_datetime(\"2024-10-03\").date()]\n",
    "\n",
    "df_filtered.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pour améliorer la visualisation, on va filtrer et retirer deux points aberrants\n",
    "\n",
    "df_max_errors = df_final[df_final[\"Total_Errors\"] == df_final[\"Total_Errors\"].max()]\n",
    "#df_max_errors.head()\n",
    "\n",
    "df_filtered = df_final[~(df_final['Date et heure'] == '2024-08-29 11:00:00')]\n",
    "\n",
    "df_max_errors = df_filtered[df_filtered[\"Total_Errors\"] == df_filtered[\"Total_Errors\"].max()]\n",
    "df_max_errors.head()\n",
    "\n",
    "df_filtered = df_filtered[~(df_filtered['Date et heure'] == '2024-10-03 17:00:00')]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "\n",
    "# Création de la figure et des axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Axe principal (opérations normales)\n",
    "ax1.plot(df_filtered[\"Date et heure\"], df_filtered[\"nb_operations\"], marker=\"o\", linestyle=\"-\", label=\"Nb opérations normales\", color=\"blue\")\n",
    "ax1.set_xlabel(\"Heure\")\n",
    "ax1.set_ylabel(\"Nb opérations normales\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "# Création d'un second axe Y pour les erreurs\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_filtered[\"Date et heure\"], df_filtered[\"Total_Errors\"], marker=\"s\", linestyle=\"--\", label=\"Nb opérations erreurs\", color=\"red\")\n",
    "ax2.set_ylabel(\"Nb opérations erreurs\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "# Titre et légende\n",
    "plt.title(\"Comparaison des opérations normales et erreurs par heure\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# Rotation des dates pour meilleure lisibilité\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyse des ETL les plus présents dans les 2 logs"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "top_program = df_error['Program_Name'].value_counts()\n",
    "top_program = top_program.head()\n",
    "top_program.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "top_program = df['Program_Name'].value_counts()\n",
    "top_program = top_program.head()\n",
    "top_program.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion des DF de stat server"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_reseau = pd.read_csv('../data/logServer/filtered/myreport_reseau_filtered.csv')\n",
    "df_sql_statistic = pd.read_csv('../data/logServer/filtered/myreport_sql_statistic_filtered.csv')\n",
    "df_sql_lock = pd.read_csv('../data/logServer/filtered/myreport_sql_lock_filtered.csv')\n",
    "df_sql_general = pd.read_csv('../data/logServer/filtered/myreport_sql_general_filtered.csv')\n",
    "df_ping = pd.read_csv('../data/logServer/filtered/myreport_ping_filtered.csv')\n",
    "df_storage = pd.read_csv('../data/logServer/filtered/myreport_espace_disque_filtered.csv')\n",
    "df_swap = pd.read_csv('../data/logServer/filtered/myreport_swap_filtered.csv')\n",
    "df_sql_management_storage = pd.read_csv('../data/logServer/filtered/myreport_sql_gestionairedememoire_filtered.csv')\n",
    "df_ram = pd.read_csv('../data/logServer/filtered/myreport_ram_filtered.csv')\n",
    "df_cpu = pd.read_csv('../data/logServer/filtered/myreport_cpu_filtered.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_cpu.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dfs = [df_reseau, df_sql_statistic, df_sql_lock, df_sql_general, df_ping, df_storage, df_swap, df_sql_management_storage, df_ram, df_cpu]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_sql_general.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from functools import reduce\n",
    "\n",
    "df_server_stats = reduce(lambda left, right: pd.merge(left, right, on=\"Date et heure\", how=\"outer\"), dfs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_server_stats.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_server_stats.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_server_stats_sansna = df_server_stats.dropna()\n",
    "\n",
    "print(df_server_stats.shape)\n",
    "print(df_server_stats_sansna.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_server_stats.isna().sum().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion du df conso logs + df conso stat server"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_final[\"Date et heure\"].dtype)\n",
    "print(df_server_stats[\"Date et heure\"].dtype)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_server_stats['Date et heure'] = pd.to_datetime(df_server_stats['Date et heure'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_server_stats.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_final[\"Date et heure\"].dtype)\n",
    "print(df_server_stats[\"Date et heure\"].dtype)\n",
    "df_server_stats.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fusion des deux df conso\n",
    "df_global = df_final.merge(\n",
    "    df_server_stats,\n",
    "    on=\"Date et heure\",\n",
    "    how=\"outer\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_global.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_global.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_global.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_global = df_global.fillna(0)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_global.to_csv(\"../data/dataset_LogETL_LogServer.csv\", index=False, encoding='utf-8')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DORenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
