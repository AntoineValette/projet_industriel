name: smartETL

services:
  postgres:
    image: postgres:17.2
    container_name: dor_postgres
    env_file:
      - .env
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgresql/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: always
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      dor_network:
        aliases:
          - dor_network

  pgadmin:
    image: dpage/pgadmin4:8.14.0
    container_name: dor_pgadmin
    env_file:
      - .env
    ports:
      - "5050:80"
    depends_on:
      postgres:
          condition: service_healthy
    volumes:
      - ./conf/postgresql/servers.json:/pgadmin4/servers.json
    networks:
      dor_network:
        aliases:
          - dor_network

  python_app:
    container_name: dor_scripts
    build:
      context: .
      dockerfile: pythonApp.Dockerfile
    env_file:
      - .env
    depends_on:
      postgres:
          condition: service_healthy
    volumes:
      - ./data:/data
    entrypoint: ["python", "smartETL.py"]
    networks:
      dor_network:
        aliases:
          - dor_network

  # Kafka Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: my_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      dor_network:
        aliases:
          - dor_network

  kafka-manager:
    image: hlebalbau/kafka-manager:3.0.0.5
    container_name: kafka-manager
    ports:
      - "9000:9000"
    environment:
      ZK_HOSTS: "zookeeper:2181"
      APPLICATION_SECRET: "randomsecret"
    depends_on:
      - zookeeper
      - kafka
    networks:
      dor_network:
        aliases:
          - dor_network

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: my_kafka
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_MESSAGE_MAX_BYTES: 209715200  # Augmente la taille maximale des messages à 200 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 209715200  # Augmente la taille maximale des données récupérées par les réplicas
      KAFKA_FETCH_MESSAGE_MAX_BYTES: 209715200  # Augmente la taille maximale des messages récupérés par les consommateurs
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    networks:
      dor_network:
        aliases:
          - dor_network

  # Python Producer
  producer:
    build:
      dockerfile: kafka.Dockerfile
      context: .
    container_name: my_python_producer
    depends_on:
      - kafka
      - postgres
    env_file:
      - .env
    volumes:
      - ./data:/data
    entrypoint: ["python", "producer.py"]
    networks:
      dor_network:
        aliases:
          - dor_network

  # Python Consumer (for consuming Kafka messages and inserting into PostgreSQL)
  consumer:
    build:
      dockerfile: kafka.Dockerfile
      context: .
    container_name: my_python_consumer
    depends_on:
      - kafka
      - postgres
    entrypoint: ["python", "consumer.py"]
    env_file:
      - .env
    networks:
      dor_network:
        aliases:
          - dor_network

  grafana:
    image: grafana/grafana:11.5.1
    container_name: dor_grafana
    ports:
      - "3000:3000"
    env_file:
      - .env
    volumes:
      - grafana-data:/var/lib/grafana
      - ./conf/grafana/provisioning:/etc/grafana/provisioning
      - ./conf/grafana/dashboards:/import
    depends_on:
      postgres:
          condition: service_healthy
    networks:
      dor_network:
        aliases:
          - dor_network

volumes:
  postgres-data:
  grafana-data:

networks:
  dor_network:
